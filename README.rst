GEOMETOR â€¢ elements
===================

Mission: codify Euclid's Elements

This project originally began on the Grav CMS. XML content of the Heath Edition of Elements was downloaded from the Perseus project. It was parsed and cleaned up into markdown files. Images of the constructions from another version by Casey were turned into transparent pngs for all entries in books 1-6 & 11. There are some discrepancies in the labels on the elements.

you can see all of this original content in the `resources/` folder

`resources/grav_md` has all the work done for markdown translation

`resources/xml/split` has all of the individual xml entries split into separate files.

the ultimate goal is to render all the principles of Euclid Element in GEOMETOR explorer. So the first step is to formalize the operations of a proof, recognize the elements and the goal - and most importantly, understand and model the dependency chain.

The next step is to reprocess the xml files into our sphinx `docsrc/` folder where we can do a great deal with cross-referencing and indexing. 

In `docsrc/elements/01/` you will see a first attempt at the parsing - "constructing an equilateral triangle" has a first attempt at a code like sequence with images generated by GEOMETOR. But I think we can do a better job with the abilties of our new explorer interface - interactive.

I am less interested in Euclid's hierarchy of numbered books and propositions and more interested in the grouping of concepts and their place on the dependency tree.

I want each principle to have a succinct title.

We will also be consistent with the definition and attributes of the elements in GEOMETOR model.


Image Processing & Text Ingestion Workflow
-----------------------------------------

A comprehensive, modular pipeline (`src/geometor/elements/ingest`) has been established to process Heath's Euclid from PDF sources.
This workflow is divided into two main stages:

1.  **Ingestion Pipeline (`pipeline.py`)**: This stage focuses on initial extraction and organization.

    *   **Extraction (`extraction.py`)**: Converts PDF pages into images and extracts text. It also identifies and extracts the Table of Contents for each volume.
    *   **Organization (`organization.py`)**: Structures the extracted files (images, text, and manifest JSONs) into a logical hierarchy within `resources/heath/volume_X/` based on the Table of Contents.

2.  **Refinement Pipeline (`refine.py`)**: This stage performs further analysis and processing on the ingested data.

    *   **Analysis (`analysis.py`)**: Scans the extracted text to identify and index propositions, recording their start and end points within pages.
    *   **Cropping (`cropping.py`)**: Dynamically crops proposition text and extracts geometric diagrams from the high-resolution page images. It can stitch multi-page propositions into single images. Cropped propositions are saved in `resources/heath/cropped/` and extracted graphics in `resources/heath/graphics/`.
    *   **Metadata Extraction (`metadata_extraction/metadata_extraction.py`)**: Extracts and compiles metadata from the processed content for use in documentation generation.

This modular approach ensures flexibility and maintainability, streamlining the preparation of all materials for use in the GEOMETOR explorer and Sphinx-based documentation.


Contributing
------------

Contributions are welcome! Please see our [GitHub issues](https://github.com/geometor/euclid/issues) for ways to contribute.

License
-------

**euclid** is licensed under the MIT License. See the `LICENSE` file for more details.
